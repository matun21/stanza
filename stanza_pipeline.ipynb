{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# –ü–æ–ª–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏–∏ Stanza\n",
    "–ê–Ω–∞–ª–æ–≥ Streamlit-–ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è –≤ Jupyter Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. –£—Å—Ç–∞–Ω–æ–≤–∫–∞ (–µ—Å–ª–∏ –Ω–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–æ —Ä–∞–Ω–µ–µ)\n",
    "!pip install stanza\n",
    "!python -m stanza.download ru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏\n",
    "import stanza\n",
    "\n",
    "nlp = stanza.Pipeline('ru', processors='tokenize,lemma')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. –¢–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ (–∫–∞–∫ –≤ Streamlit-–ø—Ä–∏–ª–æ–∂–µ–Ω–∏–∏)\n",
    "examples = [\n",
    "    \"–ë–µ–≥—É—â–∏–µ –ª–∏—Å—ã –ø–µ—Ä–µ–ø—Ä—ã–≥–∏–≤–∞–ª–∏ —á–µ—Ä–µ–∑ –≤—ã—Å–æ–∫–∏–µ –∑–∞–±–æ—Ä—ã\",\n",
    "    \"–ö—Ä–∞—Å–∏–≤—ã–µ —Ü–≤–µ—Ç—ã —Ä–∞—Å–ø—É—Å—Ç–∏–ª–∏—Å—å –≤ –Ω–∞—à–∏—Ö —Å–∞–¥–∞—Ö\",\n",
    "    \"–î–µ—Ç–∏ –≤–µ—Å–µ–ª–æ –∏–≥—Ä–∞–ª–∏ –≤–æ –¥–≤–æ—Ä–µ\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. –§—É–Ω–∫—Ü–∏—è –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏–∏ (–∞–Ω–∞–ª–æ–≥ Streamlit)\n",
    "def lemmatize_text(text):\n",
    "    doc = nlp(text)\n",
    "    return ' '.join([word.lemma for sent in doc.sentences for word in sent.words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. –ü–∞–π–ø–ª–∞–π–Ω –æ–±—Ä–∞–±–æ—Ç–∫–∏\n",
    "for example in examples:\n",
    "    print(f\"\\nüìå –ü—Ä–∏–º–µ—Ä: {example}\")\n",
    "    \n",
    "    # –õ–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è\n",
    "    lemmas = lemmatize_text(example)\n",
    "    print(f\"üîπ –õ–µ–º–º—ã: {lemmas}\")\n",
    "    \n",
    "    # –î–µ—Ç–∞–ª—å–Ω—ã–π —Ä–∞–∑–±–æ—Ä (–∫–∞–∫ –≤ Streamlit)\n",
    "    doc = nlp(example)\n",
    "    print(\"üîπ –î–µ—Ç–∞–ª–∏–∑–∞—Ü–∏—è:\")\n",
    "    for sent in doc.sentences:\n",
    "        for word in sent.words:\n",
    "            print(f\"{word.text:15} ‚Üí {word.lemma:15} (POS: {word.pos})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. –ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π —Ä–µ–∂–∏–º\n",
    "print(\"\\nüîß –ü—Ä–æ–≤–µ—Ä—å—Ç–µ —Å–≤–æ–π —Ç–µ–∫—Å—Ç:\")\n",
    "user_text = input(\"–í–≤–µ–¥–∏—Ç–µ —Ç–µ–∫—Å—Ç: \")\n",
    "\n",
    "if user_text.strip():\n",
    "    print(f\"\\n–†–µ–∑—É–ª—å—Ç–∞—Ç: {lemmatize_text(user_text)}\")\n",
    "else:\n",
    "    print(\"–í—ã –Ω–µ –≤–≤–µ–ª–∏ —Ç–µ–∫—Å—Ç\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –ö–∞–∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å:\n",
    "1. –ó–∞–ø—É—Å—Ç–∏—Ç–µ –≤—Å–µ —è—á–µ–π–∫–∏ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ (Kernel ‚Üí Restart & Run All)\n",
    "2. –î–ª—è —Å–≤–æ–µ–≥–æ —Ç–µ–∫—Å—Ç–∞ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ –ø–æ—Å–ª–µ–¥–Ω—é—é —è—á–µ–π–∫—É\n",
    "3. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –±—É–¥—É—Ç –≤—ã–≤–æ–¥–∏—Ç—å—Å—è –ø–æ–¥ –∫–∞–∂–¥–æ–π —è—á–µ–π–∫–æ–π"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 }
}
